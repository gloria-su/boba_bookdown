# Methodology and Results
## Statistical Tests 
Prior to conducting any statistical tests, we checked the assumptions for normality, equal variance, and independence. Since we had a relatively small data set, we used the Shapiro-Wilk test to assess whether the shops were normally distributed and Levene's test for equality of variances (F-test) between shops in Raleigh and NYC. While the normality assumption did not hold (p= 3.81 x 10-4), we were able to confirm the equality of variances (p= 0.81). For the independence assumption, we assumed each shop was independent; however, we created a feature called “chain” that provided a binary flag if there were multiple locations. 

Thus, we performed a Wilcoxon's 2 sample t-test on the two different groups. We found that boba shops in Raleigh were significantly different from boba shops in NYC, with a p-value of 4e-03. All statistical tests utilized a significance level (alpha) of 0.05. The mean rating of shops in Raleigh was 4.16 (variance 0.32), while the mean rating in NYC was 3.84 (variance 0.35). This resulted in a difference in means of 0.35.

With our intial hypothesis confirmed by the statistically significant difference, our focus shifted to a more detailed comparison of boba shops in Raleigh and NYC. We decided to utilize both an explanatory and predictive approach, running a linear regression model for explainability and a random forest model for predictability.

## Linear Regression Model
To utilize an explanatory approach, we built two different variable selection models with rating as the target variable using forward stepwise regression. One model was for Raleigh while the other model was for New York City. After setting a full model and an empty model separately for the Raleigh and NYC data sets, we ran stepwise selection with k, the number of model parameters, set to 2. The following code chunks display the stepwise selection code.

### Raleigh Variable Selection 
```{r, echo=FALSE, message=FALSE}
boba2 <- boba[,c(1, 3, 6, 8, 12:41)]
boba2[, c(3,5)] <- lapply(boba2[, c(3,5)], function(col) factor(col))
boba2[, c(6:34)] <- lapply(boba2[, c(6:34)], function(col) factor(as.numeric(col)))

#import library
library("tidyverse")
#filter to only Raleigh variable selection
raleigh_df <- boba2[boba2$source=="raleigh_boba",]
raleigh_df <- raleigh_df[,-c(5,23:32)]
# summary(raleigh_df[, c(2:34)])

#Do forward selection for the columns that explain rating
full.model <- lm(Rating ~ ., data = raleigh_df[,c(2:23)])
empty.model <- lm(Rating ~ 1, data = raleigh_df)
for.model.raleigh <- step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "forward", k = 2,trace = FALSE)
summary(for.model.raleigh)
# Rating ~ Juice.Bars...Smoothies_ind + Asian.Fusion_ind + Coffee...Tea_ind + Bakeries_ind + Tea.Rooms_ind <- not sig
```

### NYC Variable Selection
```{r, echo=FALSE}
#filter to only NYC variable selection
nyc_df <- boba2[boba2$source=="nyc_boba",]
nyc_df <- nyc_df[,-c(5,9,16:19,22,33:34)]
# summary(nyc_df[, c(2:34)])

#Do forward selection for the columns that explain rating
full.model.nyc <- lm(Rating ~  ., data = nyc_df[,c(2:25)])
empty.model.nyc <- lm(Rating ~ 1, data = nyc_df)
for.model.nyc <- step(empty.model.nyc, scope = list(lower = empty.model.nyc, upper = full.model.nyc), direction = "forward", k = 2, trace=FALSE)
summary(for.model.nyc)
# Authenticity + City + Creperies_ind + Taiwanese_ind + Juice.Bars...Smoothies_ind
```

The linear regression for Raleigh that had the lowest AIC of -67.90 included three features of the juice bar/smoothie indicator, Asian fusion indicator, and coffee/tea indicator. The best linear regression for NYC had an AIC of -87.14 and found the features of authenticity, city, creperies, Taiwanese, and juice bar/smoothie indicator. Among the two sites, there were seven distinct features observed between the two variable selection models. The only shared feature was the juice bar/smoothie indicator, leading us to wonder if the random forest features would be similar.

## Random Forest Model
While random forest models are generally used for prediction, we used their ability to assess variable importance as a comparison point for the linear regression model. Our feature ranking was determined by the percent increase in the mean squared error (MSE). We assessed the variable importance of our predictive model output with a variable called “random”, seen in the plots below.

The Raleigh plot of variable importance is displayed below:
```{r, echo=FALSE}
#import library
library(randomForest)
#Run a random forest model for Raleigh
set.seed(444)
raleigh_df$random<- rnorm(43)
set.seed(444)
rf.raleigh <- randomForest(Rating ~ ., data = raleigh_df, ntree = 250, importance = TRUE)
varImpPlot(rf.raleigh ,
           sort = TRUE,
           n.var = 10,
           main = "Top 10 - Variable Importance for Raleigh", type = 1)
```

The top ten important variables for the predictive model in Raleigh were similar to the linear regression model. Of the three explanatory variables of the linear regression (juice bar/smoothie indicator, Asian fusion indicator, coffee/tea indicator), only juice bar/smoothie appeared before the random variable. A high overlap between explanatory and predictor variables allows us to infer that there are similar attributes that go into forming and retaining a good Raleigh boba shop. These include:

- Having a variety of options, such as smoothies and juices.

- Appealing to the “fusion” audience rather than as an authentic boba shop.

- Being tagged as a tea room.

- Having food options, such as baked goods.

- Offering coffee choices.

The NYC plot of variable importance is displayed below:
```{r, echo=FALSE}
set.seed(444)
nyc_df$random<- rnorm(70)
set.seed(444)
rf.nyc <- randomForest(Rating ~ ., data = nyc_df, ntree = 250, importance = TRUE)
varImpPlot(rf.nyc,
           sort = TRUE,
           n.var = 10,
           main = "Top 10 - Variable Importance for NYC", type = 1)
```

The top importance variables output from the random forest for New York City seemed to focus on location, seen in the ranking of the “city” feature.

It’s important to note that the “city” variable was important in both the Raleigh and NYC variable importance outputs from our random forests; this may indicate that the shop’s location is a large factor in a Yelp rating. This output prompted us to visualize our dataset and explore the map of boba shops across both data sets.

## Visualization
We utilized the visualization platform Tableau to display both Raleigh and NYC boba shops by longitude and latitude. The feature "city" in the data set indicated that Raleigh’s n = 4 boba shops spanned 9 towns, while NYC’s n = 70 boba shops spanned 10 neighborhoods. We noticed that shops in NYC tended to be more concentrated or in closer proximity to each other than shops in Raleigh. This indicates that there was more competition in close proximity in NYC than in Raleigh. This visualization reiterated the results from both the variable selection model and the predictive random forest model. The variable selection model displayed the most significant explanatory variables, indicating a gap in features between locations.

The random forest model confirmed that the "city" feature was important in mean boba shop rating outcomes, which could be attributed in part to physical shop proximity, as seen on the Tableau dashboard.

